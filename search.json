[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Flight Free\n\n\n\n\n\n\n\nclimate\n\n\n\n\nQuantifying personal CO2 emissions with Google Timeline and other thoughts\n\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRecommender Systems with Metaflow\n\n\n\n\n\n\n\nml\n\n\ntalk\n\n\n\n\nA talk I gave with a colleague whilst working at Too Good To Go about the benefits of effective ML infrastructure\n\n\n\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRecSys 2022 - My Top 5 Papers\n\n\n\n\n\n\n\nml\n\n\nrecsys\n\n\n\n\nHighlights from the conference\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDebugging for Dockerized ML Applications in Python\n\n\n\n\n\n\n\nml\n\n\npython\n\n\n\n\nUsing VScode and debugpy to make debugging a breeze\n\n\n\n\n\n\nJul 31, 2021\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Custom Semantic Segmentation Model\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\n\n\nUsing your own data to create a robust computer vision model\n\n\n\n\n\n\nNov 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSolving Futoshiki Puzzles in Python\n\n\n\n\n\n\n\npython\n\n\ncomputer vision\n\n\n\n\nIs it possible to automate solving a Futoshiki puzzle from a webcam image?\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html",
    "href": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "section": "",
    "text": "Course Page: https://karpathy.ai/zero-to-hero.html"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#setup",
    "href": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#setup",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "section": "Setup",
    "text": "Setup\n\n# there no change change in the first several cells from last lecture\n\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n\n\n# read in all the words\nwords = open('names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n  X, Y = [], []\n  \n  for w in words:\n    context = [0] * block_size\n    for ch in w + '.':\n      ix = stoi[ch]\n      X.append(context)\n      Y.append(ix)\n      context = context[1:] + [ix] # crop and append\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\n# ok biolerplate done, now we get to the action:\n\n\n# utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff} | shapes: {dt.shape} vs. {t.shape}')\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nparameters_named = dict(\n  C=C,\n  W1=W1,\n  b1=b1,\n  W2=W2,\n  b2=b2,\n  bngain=bngain,\n  bnbias=bnbias,\n)\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n4137\n\n\n\nbatch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n\n# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\nloss\n\ntensor(3.3390, grad_fn=&lt;NegBackward0&gt;)"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-the-atomic-compute-graph",
    "href": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-the-atomic-compute-graph",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "section": "Backprop through the atomic compute graph",
    "text": "Backprop through the atomic compute graph\n\nfrom torchviz import make_dot\n\nmake_dot(loss, params=parameters_named)\n\n\n\n\n\n# Exercise 1: backprop through the whole thing manually, \n# backpropagating through exactly all of the variables \n# as they are defined in the forward pass above, one by one\n\ndlogprobs = torch.zeros_like(logprobs)\n# get all rows, and index into the correct column for the labels\ndlogprobs[range(n), Yb] = -1/n\ndprobs = dlogprobs * 1 / probs\ndcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)\ndcounts_sum = dcounts_sum_inv * -counts_sum**-2\ndcounts = counts_sum_inv * dprobs\n# gradients flow through other vertex!\ndcounts += dcounts_sum * 1\n\ndnorm_logits = dcounts * norm_logits.exp()\ndlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n    \ndlogits = dnorm_logits.clone() \ndlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n\ndh = dlogits @ W2.T\n\ndW2 = h.T @ dlogits\ndb2 = dlogits.sum(0)\n\ndhpreact = dh * (1 - h**2)\n\ndbngain = (dhpreact * bnraw).sum(0, keepdim=True)\ndbnbias = dhpreact.sum(0, keepdim=True)\ndbnraw = dhpreact * bngain\n\ndbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\ndbndiff = bnvar_inv * dbnraw\ndbnvar = dbnvar_inv * -.5 * (bnvar + 1e-5)**-1.5\n\ndbndiff2 = torch.ones_like(bndiff2) * 1 / (n-1) * dbnvar\ndbndiff += (2*bndiff) * dbndiff2\ndhprebn = dbndiff.clone()\n\ndbnmeani = (-dbndiff).sum(0, keepdim=True)\ndhprebn += dbnmeani * torch.ones_like(hprebn) * 1 / n\n\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = torch.ones_like(emb) * dembcat.view(n, 3, -1) \n\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]):\n    for j in range(Xb.shape[1]):\n        ix = Xb[k,j]\n        dC[ix] += demb[k,j]\n\ncmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)\n\nlogprobs        | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 27]) vs. torch.Size([32, 27])\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 27]) vs. torch.Size([32, 27])\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 1]) vs. torch.Size([32, 1])\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 1]) vs. torch.Size([32, 1])\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 27]) vs. torch.Size([32, 27])\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 27]) vs. torch.Size([32, 27])\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 1]) vs. torch.Size([32, 1])\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 27]) vs. torch.Size([32, 27])\nh               | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nW2              | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([64, 27]) vs. torch.Size([64, 27])\nb2              | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([27]) vs. torch.Size([27])\nhpreact         | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nbngain          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([1, 64]) vs. torch.Size([1, 64])\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([1, 64]) vs. torch.Size([1, 64])\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nbnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([1, 64]) vs. torch.Size([1, 64])\nbnvar           | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([1, 64]) vs. torch.Size([1, 64])\nbndiff2         | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nbndiff          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nbnmeani         | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([1, 64]) vs. torch.Size([1, 64])\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 64]) vs. torch.Size([32, 64])\nembcat          | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 30]) vs. torch.Size([32, 30])\nW1              | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([30, 64]) vs. torch.Size([30, 64])\nb1              | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([64]) vs. torch.Size([64])\nemb             | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([32, 3, 10]) vs. torch.Size([32, 3, 10])\nC               | exact: True  | approximate: True  | maxdiff: 0.0 | shapes: torch.Size([27, 10]) vs. torch.Size([27, 10])"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-cross-entropy-in-one-go",
    "href": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-cross-entropy-in-one-go",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "section": "Backprop through cross-entropy in one go",
    "text": "Backprop through cross-entropy in one go\n\nComputation graph"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-batchnorm-in-one-go",
    "href": "notes/karpathy-zero-to-hero/makemore_4_backprop_ninja.html#backprop-through-batchnorm-in-one-go",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "section": "Backprop through Batchnorm in one go",
    "text": "Backprop through Batchnorm in one go\n\nComputation graph\n\n\n\n\n\n\n\nG\n\n  \n\nx\n\n x   \n\nμ\n\n μ   \n\nx-&gt;μ\n\n    \n\nx̂\n\n x̂   \n\nx-&gt;x̂\n\n    \n\nσ\n\n σ   \n\nx-&gt;σ\n\n    \n\nμ-&gt;x̂\n\n    \n\nμ-&gt;σ\n\n    \n\ny\n\n y   \n\nx̂-&gt;y\n\n    \n\nσ-&gt;x̂\n\n   \n\n\n\n\n\n\n\nForward Pass Equations\ngiven: \\[\n    \\displaylines{\\mu = \\frac{1}{n}\\sum_{i}^{n}x_i \\\\\n    \\sigma^2 = \\frac{1}{n-1}\\sum_{i}^{n}(x_i - \\mu)^2\n    }\n\\]\n\n\n(note Bessel’s correction)\nthen: \\[\n    \\hat{x_i} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\n\\] and finally: \\[y_i = \\gamma \\hat{x_i} + \\beta\\]\n\n\nBackward Pass Derivation"
  },
  {
    "objectID": "posts/dockerized-debugging-ml/index.html",
    "href": "posts/dockerized-debugging-ml/index.html",
    "title": "Debugging for Dockerized ML Applications in Python",
    "section": "",
    "text": "Docker has become ubiquitous in ML applications in the last few years. It can enable easy collaboration between engineers with different hardware — as well as easing the transition from prototyping on personal laptops to compute clusters in production. On the flip side, it introduces an extra layer of complexity for engineers to work with when developing and maintaining productionised models.\nIn my professional work, I’ve found debugging to be one of the things made harder by this extra layer of complexity. In this post I’m going to outline my current setup with VSCode and debugpy that greatly simplifies this process when applied to a model training application."
  },
  {
    "objectID": "posts/dockerized-debugging-ml/index.html#why-do-we-need-this",
    "href": "posts/dockerized-debugging-ml/index.html#why-do-we-need-this",
    "title": "Debugging for Dockerized ML Applications in Python",
    "section": "Why do we need this?",
    "text": "Why do we need this?\nWhen debugging code, we want to be able to inspect its environment at runtime as accurately as possible. Any deviation from this can lead to fixes that plainly don’t work in the runtime environment.\nWhilst getting to grips with Docker, my debugging process would generally entail recreating the scaffolding around the Python script that I wanted to inspect in my local development environment, and then debugging that script directly. In model training applications that contain bash scripts, multiple entrypoints and use of environment variables, this can quickly add a large development overhead. With this added complexity comes the increased chance of errors creeping in, rendering the whole process slow and frustrating.\nSo how do we avoid this? We need a debugging system that can interact with Docker, and let our code run as it was designed to!"
  },
  {
    "objectID": "posts/dockerized-debugging-ml/index.html#the-solution",
    "href": "posts/dockerized-debugging-ml/index.html#the-solution",
    "title": "Debugging for Dockerized ML Applications in Python",
    "section": "The Solution",
    "text": "The Solution\nWhat worked best in the end for me is a debugger that can connect to a Docker container where your model training application is running, and directly inspect the environment of a given Python script.\nThis is where debugpy comes in!\nPreviously known as ptvsd, this package is developed by Microsoft specifically for use in VSCode with Python. It implements all of the common debugging tools you would expect, as well as allowing for attaching to remote environments, such as Docker containers or even remote machines via SSH.\nAs an aside, debugpy implements the Debug Adapter Protocol (DAP), which is a standardised way for development tools to communicate with debuggers.\nUsing debugpy with Docker containers is wonderfully simple, and requires 3 distinct steps. I’ll dive into each of these in turn, before demonstrating the whole process afterwards.\n\nConfiguring debugpy in Python\nConfiguring the connection to the Docker container\nSetting up breakpoints\n\n\n1. Configuring debugpy in Python\nIn the script you would like to debug, the following snippet should be added before any other code.\nimport debugpy\n\ndebugpy.listen((\"0.0.0.0\", 5678))\nprint(\"Waiting for client to attach...\")\ndebugpy.wait_for_client()\nThis will setup debugpy to listen on port 5678 for a client to attach, and will also pause the execution until a client connects via that port.\n\n\n2. Configuring the connection to the Docker container\nNow we have our Python script configured, we need to make sure the VSCode debugger client can connect to debugpy when it is running inside a Docker container.\nFirstly, when you run your Docker container, the port that debugpy is listening on must be mapped to a local port\ndocker run \\\n    -p 5678:5678 \\  # map container port to local port\n    temp-container\nSecondly, we need to create a launch.json file to configure how the local VSCode debugging client will run. This minimal example tells the debugger to attach to port 5678, which will be mapped to the Docker port of the same number when we run the container.\n{\n   \"version\":\"0.2.0\",\n   \"configurations\":[\n      {\n         \"name\":\"Python: Docker Attach\",\n         \"type\":\"python\",\n         \"request\":\"attach\",\n         \"connect\":{\n            \"host\":\"localhost\",\n            \"port\":5678\n         },\n         \"pathMappings\":[\n            {\n               \"localRoot\":\"${workspaceFolder}\",\n               \"remoteRoot\":\".\"\n            }\n         ]\n      }\n   ]\n}\n\n\n3. Setting up breakpoints\nI was surprised when I first tried this that when you set breakpoints via the VSCode UI on the local version of a Python script, that will correspond to the copied scripts that run inside your Docker container! Pure wizardry from VSCode.\nIn addition, you can also use debugpy.breakpoint() to explicitly set breakpoints via the debugpy API. An additional benefit of this is that these calls will be ignored if you exclude the debugpy configuration mentioned in step (1), providing a quick way of temporarily removing debugging."
  },
  {
    "objectID": "posts/dockerized-debugging-ml/index.html#debugging-in-action",
    "href": "posts/dockerized-debugging-ml/index.html#debugging-in-action",
    "title": "Debugging for Dockerized ML Applications in Python",
    "section": "Debugging in action",
    "text": "Debugging in action\nYou should be good to go! The steps to debug are:\n\nAdd breakpoints in the UI\nRebuild and run the Docker container\nConnect the debugger\n\n\nThe full code used for this example can be found below. Happy debugging! 😃\nGitHub - sam-watts/vscode-docker-debugging: A template for debugging long running, dockerized programs in python with vscode"
  },
  {
    "objectID": "posts/dockerized-debugging-ml/index.html#links",
    "href": "posts/dockerized-debugging-ml/index.html#links",
    "title": "Debugging for Dockerized ML Applications in Python",
    "section": "Links",
    "text": "Links\n\ndebugpy Github: https://github.com/microsoft/debugpy\nDocker run reference: https://docs.docker.com/engine/reference/run/"
  },
  {
    "objectID": "posts/google-timeline-emissions/index.html",
    "href": "posts/google-timeline-emissions/index.html",
    "title": "Flight Free",
    "section": "",
    "text": "&gt;&gt;&gt; Dashboard to analyse your own Google Timeline data &lt;&lt;&lt;\nA few weeks ago I came across this image and found the relative impact of flying versus avoiding meat striking. I’ve followed a vegan diet for the past 5 years, and try my best to take the train when travelling across europe, so I have some awareness of my personal emissions. I think sometimes it’s easy for these numbers to get lost in the wider conversation about climate change.\nThis image got me thinking about trying to work out how much of my own transport emissions were coming from flights, and how that stacks up against the average person.\nBeing a lazy programmer at heart, I decided to try and use Google Timeline to gather data on my past flights, rather than manually recording my flights from the past n years.\nGoogle Timeline tracks your phone location, storing start and end locations for journeys between any two locations. Based on this, it estimates your method of transport with a confidence score (presumably using a machine learning model) and also calculates the distance.\nWhen we know the distance and method of transport of a journey, we can apply some emissions factors to calculate the grams of CO2 equivalent produced for that journey. For our purposes, I used figures sourced by Oliver Corradi in his general post about climate change1.\nIf you have Google Timeline enabled in the Google Maps app, you can download your data from Google Takeout. I decided to create a dashboard to visualize my own data, and make it available so that others can do the same: https://transport-co2-emissions.streamlit.app/\nMy transport emissions were highest in 2017, where I took 13 flights, totalling around 25,000km. This puts my total transport emissions at more than 3x the UK and almost 2x the US average. In this year flying was responsible for 75% of my transport emissions.\nLooking at the breakdown of individual flights, all of them are short to medium haul in the last few years, with the furthest flight being from the UK to Egypt.\nAlthough my flight emissions were higher than the UK average, this still puts me below the 1% of the world’s most frequent flyers, who travel around 56,000km per year on average2. This group are responsible for 50% of global aviation emissions, which shows the highly non-uniform distribution of flying in the world population.\nFlying contributes 2.5% of total global carbon emissions - which is smaller than other sectors such as animal livestock (5.8%) and road transport (11.9%)3 - but the difference is that the impact is being caused by a smaller subset of the population. In wealthy subsets of the population, this generally means that flying will take up a larger proportion of an individual’s total emissions.\nWorryingly, the path towards decarbonization for aviation seems highly flawed. In the UK at least, the government seem to be mainly relying on a combination of sustainable aviation fuels (SAFs) and carbon offsets to reach net zero for aviation4.\nJust weeks ago the Royal Society published a report5 into the viability of SAFs in covering the current level of flying in the UK aviation industry, with two main conclusions:\nDecarbonization of aviation becomes even more challenging when you consider the increased demand on the electricity grid from personal vehicle and home heating electrification, which in the UK is projected to contribute to a 15% increase in demand by 2035. This is on top of other increases in demand on the electricity grid driven by the pursuit of growth in other sectors of the economy.\nCarbon offsets have their own fundamental problems - it has been shown that “more than 90% of carbon offsets by [the] biggest certifier are worthless”6\nBased on these factors it seems unrealistic to expect that the aviation industry can continue to grow at its current rate, and become net zero by 2050.\nIndividual action - ie. choosing not to fly - feels important in the context of aviation, in an industry that seems unwilling to embrace the reality of the climate crisis. Whilst I think we desperately need huge changes to our public institutions, the engagement and careful thought that comes with individual lifestyle changes feels important to me as part of building greater consciousness of change. I think Rebecca Solnit captures this best:\nReflecting on these ideas, I think there is a more general point here about when “enough is enough”. Personally, I’ve flown to many interesting places around the world already. Will it really make much of a difference to my life flying to one more, when weighed against the costs to everyone currently living, as well as future generations?\nI’ve decided to take Flight Free UK’s challenge of a flight free year for 2023. Making this decision clearly isn’t possible for people who absolutely have to fly for work, or those who need to visit close family who live in far away countries. I fully recognize the priviledge I have to be able to make this descision. Even so, I hope some of you might relate to my feelings, and consider going flight free for 2023 too."
  },
  {
    "objectID": "posts/google-timeline-emissions/index.html#footnotes",
    "href": "posts/google-timeline-emissions/index.html#footnotes",
    "title": "Flight Free",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://oliviercorradi.com/climate-change/#transportation--reduce-long-distance-travels↩︎\nhttps://www.theguardian.com/business/2020/nov/17/people-cause-global-aviation-emissions-study-covid-19 referring to https://www.sciencedirect.com/science/article/pii/S0959378020307779↩︎\nhttps://ourworldindata.org/emissions-by-sector↩︎\nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1095952/jet-zero-strategy.pdf↩︎\nhttps://royalsociety.org/topics-policy/projects/low-carbon-energy-programme/net-zero-aviation-fuels/↩︎\nhttps://www.theguardian.com/environment/2023/jan/18/revealed-forest-carbon-offsets-biggest-provider-worthless-verra-aoe↩︎\nhttps://www.theguardian.com/commentisfree/2021/aug/23/big-oil-coined-carbon-footprints-to-blame-us-for-their-greed-keep-them-on-the-hook↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Title\n\n\nDate\n\n\nCategories\n\n\n\n\n\n\nBuilding makemore Part 4: Becoming a Backprop Ninja\n\n\nJul 10, 2023\n\n\nNeural Networks: Zero to Hero,WIP\n\n\n\n\nBuilding makemore Part 3: Activations & Gradients, BatchNorm\n\n\nJul 6, 2023\n\n\nNeural Networks: Zero to Hero,WIP\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/solving-futoshiki-puzzles/index.html",
    "href": "posts/solving-futoshiki-puzzles/index.html",
    "title": "Solving Futoshiki Puzzles in Python",
    "section": "",
    "text": "Futoshiki is a type of Japanese puzzle in the same vein as Sudoku — in so far as the aim is to completely fill a grid of boxes with numbers. The grid in this case is much smaller, typically 5 x 5 rather than 9 x 9, and includes additional inequality constraints between neighbouring numbers. A couple of years back my ideal Saturday morning routine would invariably include lazily solving one from the back of the Guardian. Simpler times.\nAround this time I had also gotten into the world of data science, starting my first professional role. I knew solving the puzzle itself could be done fairly easily, but my knowledge of image processing was essentially zero. Would it be possible to feed a phone or webcam image into an algorithm that could find, process and solve the puzzle for me?\nI’ve decided to start writing up some of the coding I do in my spare time, so this will be the first of a few posts on this topic. In this post, I’m going to give an overview of the first version of the end-to-end tool that I built to do just that.\n\nCapturing the Puzzle\nThe most basic way I could think of doing this was to use a laptop webcam, allowing the captured image to be fed straight through to the rest of the algorithm locally. The function below opens up a capture window using OpenCV, showing your current webcam display, with a grid to guide capture. This allows us to crop the captured image with the grid, making life a whole lot easier down the line.\n\ndef capture_image(save_img: str=None) -&gt; Union[np.ndarray, bool]:\n    \"\"\"Capture webcam image of a futoshiki puzzle using cv2 display, with a grid guide\n    \n    :param save_img: path to write image to, defaults to None\n    :return: captured image\n    \"\"\"\n    cam = cv2.VideoCapture(0)\n    \n    while True:\n        ret, frame = cam.read()\n        height, width = frame.shape[:2]\n        grid_length = 300 \n        \n        tl = (width // 2 - grid_length // 2, height // 2 - grid_length // 2)  # top left\n        br = (width // 2 + grid_length // 2, height // 2 + grid_length // 2)  # bottom right\n        \n        output = frame.copy()\n        output = cv2.flip(output, -1)\n        cv2.rectangle(output, tl, br, (255, 0, 0), 2)\n        cv2.putText(output, 'Align puzzle with grid, then press SPACE BAR to capture image',\n                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n        \n        cv2.imshow('Futoshiki Solver Capture', output)\n        \n        if not ret:\n            break\n            \n        k = cv2.waitKey(1)\n    \n        if k % 256 == 27:\n            # ESC pressed\n            logger.info('Escape hit, closing window')\n            cv2.destroyAllWindows()\n            return False\n            \n        elif k % 256 == 32:\n            # SPACE pressed\n            # crop image to frame size - format img[y:y+h, x:x+w]\n            frame = frame[tl[1]:tl[1]+grid_length, tl[0]:tl[0]+grid_length]\n            image = cv2.flip(frame, 0)\n            if save_img:\n                if not cv2.imwrite(save_img, image):\n                    raise Exception('Couldn''t save image')\n                    logger.info(f'{save_img} written')\n                \n            cam.release()\n            return image    \n\nHitting ENTER will trigger a capture of the image, whilst ESC will cancel the capture process and close the window. I wrote some helper functions alongside this to preview the image. It helpfully flips the image around to make orienting the image for capture a bit easier!\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting Puzzle Features\nOnce we have a cropped image of the puzzle we need to find a way of extracting each box and any inequalities in a repeatable, structured way. The puzzle can then be represented by a consistent data structure to pass to the solver. For this problem I took inspiration from a great article on retrieving boxes from images. I added some of my own tweaks to this.\nOne of the trickiest things I found was getting a decent binary image — this involves removing colour and any lighting gradient from the photo, leaving the image in clear black and white. This make the subsequent task of box detection much simpler.\nTo begin with I tried Otsu’s method. This makes a key assumption that an image has a bimodal distribution of pixel values, and tries to fit the threshold in between the two bimodal peaks. For images with a clear foreground / background separation this can work well. In my case, it makes a bit of a hash of more challenging images.\n\n\n\nTwo different methods of binary thresholding of a puzzle image\n\n\nA more robust method I found was the Adaptive Gaussian Threshold. This deals effectively with variable lighting levels we are likely to encounter in a smartphone or webcam image. Essentially what this does is passes a kernel over the image. For each position of the kernel the threshold is calculated as the weighted sum of the pixel value and the kernel — otherwise known in deep learning as a convolution. The values of the kernel essentially look like a 3D Gaussian curve. As a final step, a constant is added which is useful for dealing with varying amounts of noise across the whole image.\n\n\n\nValues of a 15x15 2D Gaussian Kernel\n\n\nIn general this can add quite a bit of noise to an image, but is crucial in addressing issues seen when using other methods of thresholding. As you’ll see we can strip out this noise fairly easily later on. I tried to automate the parameter selection, with varying success. The values below do well in most of the cases I’ve tried, but it’s possible there is a much better way of setting the constant.\n\n# define a kernel size with odd side length to pass over the image\nkernel_size = int(np.asarray(img.shape).max() * 0.10 // 1)\nkernel_size = kernel_size if kernel_size % 2 != 0 else kernel_size + 1\n\n# define a constant to subtract from the calculated mean\nC = kernel_size // 10\n\nimg_bin = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                                cv2.THRESH_BINARY, kernel_size, C)\n\n\n\n\nGaussian Thresholding Result\n\n\nWe can then apply horizontal and vertical kernels to the resulting image to extract line segments that are at 90 degrees to the image. Most of the noise can be removed by applying erosion followed by dilation to each image. This is commonly known as “opening”, and works well for removing smaller clusters of pixels whilst retaining larger features in images.\n\n\n\nVertical lines extracted with a vertical kernel\n\n\n\n\n\nHorizontal lines extracted with a horizontal kernel\n\n\nThese two images can then be recombined linearly, with some optional extra “opening” if desired.\n\n\n\nFinal binarized image for feature extraction\n\n\nContours can then be extracted from this final image, using the code below. A contour is defined as any continuous curve or line with the same pixel intensity. I then subsetted these obtained shapes to only look for bounding boxes of the desired size, relative to the image.\n\n# find contours, not returning any contours inside outer ones\ncontours, _ = cv2.findContours(img_final_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n# sort by left-to-right, top-to-bottom using the bounding rectangles of contours\ncontours = sorted(contours,\n                  key=lambda ctr: cv2.boundingRect(ctr)[0] + \n                                  cv2.boundingRect(ctr)[1] * img_final_bin.shape[1] // 20)\n\n# define min and max box sizes\nbox_low = 0.05 * img.shape[0]\nbox_high = 0.25 * img.shape[0]\n\nfor c in tqdm(contours):\n    x, y, w, h = cv2.boundingRect(c)\n    if (box_low &lt; w &lt; box_high and box_low &lt; h &lt; box_high):    \n        # do further processing\n\nThe contours are also ordered by their x and y coordinates, as shown by the red boxes. This ordering lets us apply consistent logic to certain boxes to capture neighbouring inequalities — denoted by green and blue boxes.\n\n\n\nOverlayed with boxes, ready for OCR\n\n\n\n\nOptical Character Recognition (OCR)\nTesseract is an open source tool actively developed by Google for optical character recognition. I picked this as an easy off the shelf option, although it is quite fiddly to get it working, and doesn’t always produce great results. In theory you can use Tesseract to extract characters from an entire document, with the tool using it’s own internal image segmentation to achieve this.\nI used pytesseract as a simple python wrapper for the Tesseract library. The code below scans an input image of a single character, and prints the output.\n\ndef scan_image(path):\n    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n    img = cv2.imread(path)\n    val = pytesseract.image_to_string(img, config='--psm 10') # psm 10 = single character mode\n    print('Character is:', val)\n\nOn my to do list is creating a MNIST type deep learning model to do this myself, as this problem is far narrower than the one Tesseract was designed to solve.\n\n\nSolving the Puzzle\nGoogle OR-Tools is a great library for solving combinatorial optimisation problems of various formats. Professionally I’ve used most often to build and solve vehicle routing models for supply chain planning.\nHere I used a much simpler version of it for constraint programming, which is achieved by using a search process which includes propagation and backtracking. Google’s own documentation explains these concepts quite generally:\n\nPropagation — Each time the solver assigns a value to a variable, the constraints add restrictions on the possible values of the unassigned variables. These restrictions propagate to future variable assignments. Propagation can speed up the search significantly by reducing the set of variable values the solver must explore.\nBacktracking — occurs when either the solver can’t assign a value to the next variable, due to the constraints, or it finds a solution. In either case, the solver backtracks to a previous stage and changes the value of the variable at that stage to a value that hasn’t already been tried.\n\nHow does this apply to a Futoshiki? The below example of a single row captures it fairly well.\n\n\n\nA worked example of Propagation and Backtracking\n\n\nThe algorithm tries different numbers in each position, until no valid number can be placed in a particular square — in this case because 4 is not greater than 5. As each number is placed, constraints are added to that row and column so that the number cannot occur anywhere else within them. In this example, the algorithm backtracks to a previous state and tries different solutions that are not prohibited by the constraints at that earlier state.\nOnce solved, a callback passed into the solver function allows us to print solutions in a nice format.\n\n\nThe Final Result\nThe whole process currently achieves good results with a sample image that is much less noisy than those taken on my awful laptop webcam. I’m hoping to get it to work end to end with any input image soon, once I’ve learned some more image processing hacks!\n\n\n\nRunning the whole process on a sample image\n\n\nThe version of the source code referenced in this post is available on github"
  },
  {
    "objectID": "posts/recsys-2022-highlights/index.html",
    "href": "posts/recsys-2022-highlights/index.html",
    "title": "RecSys 2022 - My Top 5 Papers",
    "section": "",
    "text": "After reading papers from RecSys for several years, I was really happy to be able to (virtually) attend for the first time this year. If you haven’t heard of it, RecSys is the most important conference for new results in the field of recommender systems research.\nThis was also my first academic conference of any kind! I found the mix of academic and industry talks really balanced each other out well - it was great to see exciting theory-driven ideas alongside real world implementation stories, with all the engineering problems that come with them.\nBelow are the papers I found most interesting from the main conference, in no particular order. Links to the papers PDFs are included in the sub-titles.\n\nAugmenting Netflix Search with In-Session Adapted Recommendations\n\nBased on user research, Netflix found that users typically fall into 3 categories when searching for content: * Fetch  -  the user knows exactly what they want, and generally enter a query for an exact film or TV show * Find  -  the user broadly knows the kind of thing they want to see, but it’s not a fully formed idea - eg. comedy movies * Explore  -  the user has no fixed idea of what they are looking for, and are open to suggestions\nBased on this, Netflix reasoned that there was an opportunity to present the “Explore” users with recommendations on the pre-search page. The key point is that these recommendations would need to take into account user interactions from the current session, to align with whatever they might be looking for at the current moment. These kind of recommendations are commonly referred to as “Anticipatory Search” or “Pre-search Recommendations”.\nThe authors designed a model to provide recommendations for this use case. This model uses features such as historical user data, context about the user and the session, and well as raw sequences of in-session browsing interactions. Video metadata and embeddings are also used to provide information about the items that are interacted with.\n\nThe authors experimented with different types of deep architectures, including both dense and sparse features. This was coupled with the raw interaction sequence, which they modelled with different types of neural network modules that can accommodate sequence data - attention blocks, LSTM and GRU.\n\nThey pick out a specific example of how the model reacts to in-session browsing activity. The ranking of the recommendations is influenced by the titles browsed by the user, which they contend should result in a good experience for the user when they navigate to the search page.\n\nNo specifics are mentioned regarding which objectives are used to train the model, or how the model performs when tested online. In offline performance they see a 6% relative increase in ranking metrics against the current production model. I would be interested in hearing more about the cost-benefit tradeoff involved in deploying this model online, due to the large engineering challenge required to make real-time features available to the model at inference time.\n\n\nRecommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)\nThis paper borrows heavily from recent advances in NLP models to create a multi-purpose model for different recommendation tasks.\n\nP5 is directly inspired by the influential T5 paper. T5 applies a unified approach to transfer learning, to effectively learn multiple tasks as part of a text-to-text framework. P5 applies these concepts to recommendation tasks, as shown below.\n\nThe P5 framework allows for the use of prompt templates, which are filled with user interaction data and item metadata, to produce a fully text-based input based on the task described in the prompt. This means that all tasks can be learned simultaneously during pre-training. The authors present very promising results for this architecture compared to other state of the art models. If they can be replicated, this could lead to the use of more multi-purpose, pre-trained models in industry. I for one, look forward to becoming a rockstar prompt engineer! 🚀\n\n\nTowards Psychologically-Grounded Dynamic Preference Models\nOne of the core assumptions in many recommender models is that user preferences are static. But what if a user’s preferences change due to the items we are showing the user with our recommender system? How could this feedback loop effect what a user wants over time? This paper focuses on a framework for formalising possible user preference changes due to human psychological effects.\nThe best example included is the “Mere Exposure Effect” - which states that people are more likely to develop a preference for something that they are familiar with. This was first described by Robert Zajonc in the 60s - his experiment included nonsense words on the cover of a student newspaper. When tested, on average the students who read this paper rated the words they had been exposed to as more positive-sounding compared to other nonsense words.\nHow better to explain this all, than a graph with a grumpy cat? 😻\n\nThe authors formalise the mere exposure effect applied to recommendations mathematically, as show on the left. A user’s initial preferences (πₜ) and items (ν) are both represented as vectors. In response to being recommended an item ν at time step t+1, the users preference moves from their starting preference vector, along the line the intersects the user’s baseline preference and the item vectors. This results in the updated preference vector, πₜ₊₁. The factor γ controls how far along this line the preferences move (where γ ϵ [0, 1]). The graph below the equation depicts this in a 2-dimensional preference space.\nThe authors propose similar formulations for Operant Conditioning and Hedonic Adaptation, before including a section on simulations based on these ideas. This includes discussion on how recommenders may achieve different engagement scores in the case where user preferences are dynamic.\nThe ideas in this paper feel some way from making it into most industrial settings anytime soon - but a more holistic focus on the role platforms might be playing in moulding user preferences is definitely welcome.\n\n\nReusable Self-Attention Recommender Systems in Fashion Industry Applications\nContinuing the trend of unifying models - here engineers at the fashion website Zalando present their work on creating a single recommender model architecture that can be reused for several tasks, using the now ever-present Transformer architecture.\n\nThe authors unified the training datasets previously fed into separate models, and used them to train a recommender architecture that can be re-used for 3 different tasks: outfit ranking, outfit recommendation, real-time and personalised outfit generation. For the different subtasks, small changes are made to the Transformer architecture, and boolean masking is used to hide labels not relevant to the use case the model is being trained for.\nThe inclusion of contextual data also allows the model to work with semi-cold start users, who may have fewer significant interactions with items, as well as fully-cold start users with no item interactions, who can be predicted based on contextual data alone.\nThe model is able to learn from different interaction types due to a one-hot encoding of the interaction type. In addition, a simple integer of days-since-interaction allows the model to balance long and short term interests. \nIn A/B testing the authors report increases in user engagement compared to the previous deployed algorithms of between 5–130%.\n\n\nStreaming Session-Based Recommendation: When Graph Neural Networks meet the Neighborhood\nI found this paper intriguing for the insight it gives into some methodology issues present in the recommender system literature. \nA recent focus of the RecSys field has been session-based recommendations - providing recommendations to users using in-session signals. Several deep learning model architectures have been proposed to address this task. \nThe authors of this papers compared one of these complex approaches against some more simple baselines: * VSKNN+ - a session-based nearest neighbour approach. Finds past sessions that are similar to the current session. Items that appear in these similar sessions are used as recommendation candidates. The authors add an extension to the base VSKNN algorithm, that considers each user’s past sessions * SR+ - “Sequential Rules”, a variation of association rule learning. A rule is created when an item p appears after item q in a session, where the weight of the rule is a function of the number of items between p and q in the interaction sequence. The authors again extend this method to consider each user’s past sessions * GAG - an approached based on a Graph Neural Network\n\nThe final findings are shown in the table above, are that a hybrid approach (which simply combines the recommended items produced by VSKNN+ and SR+) performs better than GAG across all metrics and datasets the authors tested. Neither of these baselines were used as comparisons in the original paper that proposed GAG.\n\nIn all, it was a great conference with so many intelligent people to learn from - I would really recommend it! I hope you found this interesting 👋"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html",
    "href": "posts/futoshiki-semantic-seg/index.html",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "",
    "text": "Following on from my previous post on solving futoshiki puzzles, I wanted to see how feasible it would be to reliably detect and segment a futoshiki puzzle grid from an image without using a clunky capture grid. It works surprisingly well even when trained on a tiny dataset!"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#so-what-is-semantic-segmentation",
    "href": "posts/futoshiki-semantic-seg/index.html#so-what-is-semantic-segmentation",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "So what is Semantic Segmentation?",
    "text": "So what is Semantic Segmentation?\nSemantic Segmentation is a step up in complexity versus the more common computer vision tasks such as classification and object detection. The goal is to produce a pixel-level prediction for one or more classes. This prediction is referred to as an image ‘mask’. The example here shows 3 overlaid masks for person, sheep, and dog represented by the different foreground colours.\n\n\n\nFrom Deepmind’s course on Deep Learning https://www.youtube.com/watch?v=_aUq7lmMfxo&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=4\n\n\nFor my task, the setup is somewhat simpler as there is only one class to predict - the puzzle grid. To train the model, we need pairs of images and masks. The images we are using are full colour, so as an array will have the shape (H, W, 3). The masks on the other hand only have a single value per pixel (1 or 0), so will have shape (H, W, 1).\n\n\n\nAn example training pair of image and mask for my model"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#labelling-your-dataset",
    "href": "posts/futoshiki-semantic-seg/index.html#labelling-your-dataset",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Labelling your dataset",
    "text": "Labelling your dataset\nHow do we get the image masks I’ve just talked about? VIA is a great tool for image labelling — it’s open source and runs in a browser from a standalone HTML file.\n\n\n\nVIA in action\n\n\nVIA lets you export labels for multiple images as a csv, with the coordinates of each polygon in json format:\n{\n    \"name\":\"polygon\",\n    \"all_points_x\":[1973,2576,2579,1964],\n    \"all_points_y\":[2792,2816,3423,3398]\n}\nI then wrote a custom pytorch dataloader, which converts the polygon json into a single channel image mask. The training image and the target mask are then passed on to the model.\nIn total I labelled 43 images, which I split 75:25 into training and validation sets. I later added an extra 7 images to serve as a test set. This might not seem like much data to be training a large neural network on - but fortunately there are some techniques we can use to get the most out of this small set of images!"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#building-the-model",
    "href": "posts/futoshiki-semantic-seg/index.html#building-the-model",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Building the model",
    "text": "Building the model\nAs this is a prototype, I wanted to see if the approach would achieve decent results without building the whole thing myself from scratch and potentially wasting a lot of effort. With that in mind, I used the awesome segmentation-models-pytorch library. The power of this library hinges on transfer learning, which means we can avoid having to train the entire network from a standing start.\n\nU-Net\nU-Net consists of a coupled encoder and decoder structure, which builds high level abstractions of input images before expanding out these abstractions to provide a pixel-level prediction.\n\n\n\nOriginal U-Net architecture https://arxiv.org/abs/1505.04597\n\n\nThe grey arrows signify residual connections between the encoder and decoder pathways. This means that at every upwards step of the decoder, the encoder matrices of the same dimensions are concatenated together with the decoder matrices. The benefits of this are twofold:\n\nAt each level of the decoder - which would otherwise only contain high level abstraction information of the image - the network is able to combine it’s learning about high and low level features, increasing the fidelity of predictions.\nResidual connections allow backpropagation during training to skip past layers, making optimisation easier. This is also crucial when training deeper models to avoid issues with vanishing gradients.\n\nThe beauty of this architecture is also that we can use a pre-trained model that has been used for a classification task - on a dataset such as ImageNet - as our encoder. Once we remove the final classification layer from this model, this can be connected to a decoder with untrained weights, and skip-connections are added to reflect the U-Net structure. This saves a lot of compute time, as our pre-trained encoder already has good parameters for building high levels abstractions of images.\nsegmentation-models-pytorch provides pre-trained weights for a number of different encoder architectures.\n\n\nEncoder — EfficientNet-B3\nGoogle AI published their EfficientNet paper in 2019 with new thinking behind how to scale up convolutional neural networks. Alongside this, the paper proposed a range of models of increasing complexity that achieve state of the art performance.\n\n\n\nEffiecientNet Accuracy vs. Size of model https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n\n\nAs a trade off between size and performance, I chose the B3 variant to use in my model.\n\n\nPutting it all together\nSpecifying these architecture choices with segmentation-models-pytorch is a breeze:\nimport segmentation_models_pytorch as smp\nimport torch\n\nENCODER = 'efficientnet-b3'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = ['grid']\nACTIVATION = 'sigmoid'\nDEVICE = torch.device('cuda:0')\n\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n).to(DEVICE)"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#data-augmentation",
    "href": "posts/futoshiki-semantic-seg/index.html#data-augmentation",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nAs the training dataset only contains 36 images, overfitting is a serious concern. If we train for multiple epochs over this small dataset, we might worry that our model will start fitting to the noise in this small dataset, leading to poor performance on out-of-sample examples. This problem can be somewhat mitigated by data augmentation. As each training image and mask pair is read into memory to pass to the model, we apply several layers of non-deterministic image processing, as shown below.\n\nimport albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n        albu.HorizontalFlip(p=0.5),\n        albu.ShiftScaleRotate(scale_limit=0.2, rotate_limit=20, shift_limit=0.2, p=0.8, border_mode=0),\n        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        albu.RandomCrop(height=320, width=320, always_apply=True),\n        albu.IAAAdditiveGaussianNoise(p=0.1),\n        albu.IAAPerspective(p=0.5),\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.9,\n        )\n    ]\n    \n    return albu.Compose(train_transform)\n\n\n\nadapted from https://github.com/qubvel/segmentation_models.pytorch…\nIt’s useful to look at an example image to see the individual effects of each of these augmentations. This is the first image from our training set:\n\nAs you can see below, most of the augmentations by themselves only provide a subtle change - however when stacked up, they add enough novelty to our training data to stop the model fitting to the noise of the base dataset\n\n\n\nEach augmentation used in isolation\n\n\n\n\n\nStacked augmentations, to be passed to the model"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#training-evaluation",
    "href": "posts/futoshiki-semantic-seg/index.html#training-evaluation",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Training & Evaluation",
    "text": "Training & Evaluation\nThe most commonly used loss function is pixel wise Cross-Entropy Loss - similar to what is used in general classification tasks. Here, we instead use Dice Loss, which was introduced to address the issue of class imbalance in semantic segmentation:\n\\(Dice Loss = 1 - \\frac{2|A ∩ B|}{|A| + |B|}\\)\nIn practice, the intersection term of this equation is approximated by calculating the element-wise product of the prediction and target mask matrices:\n\n\n\nimage from https://www.jeremyjordan.me/semantic-segmentation/\n\n\nWe also use Intersection-over-Union (IoU) as a scoring metric. This essentially looks at the overlapping over total area of both predicted and ground truth masks, which is a similar concept to Dice Loss.\nTraining regime:\n\nTrained for 40 epochs, initial learning rate = 5x10e-4\nAfter the 30th epoch, learning rate = 5x10e-5\n\n\n\n\nDice loss of the model on training and validation sets at each epoch of training\n\n\n\n\n\nIntersection-over-Union of the model on training and validation sets at each epoch of training"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#prediction",
    "href": "posts/futoshiki-semantic-seg/index.html#prediction",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Prediction",
    "text": "Prediction\nI tested the trained model on 7 held out images from my labelled dataset, and the model achieved a IOU Score = 0.94 for these images, including some with puzzles at odd angles and as a smaller part of the image.\n\n\n\n\n\n\n\n\n\n\nI also ran the model over all of the frames in a short video to see the results more visually, which was also pretty good - it also deals well with an object covering the puzzle!"
  },
  {
    "objectID": "posts/futoshiki-semantic-seg/index.html#final-results-integration-into-solver-tool",
    "href": "posts/futoshiki-semantic-seg/index.html#final-results-integration-into-solver-tool",
    "title": "Building a Custom Semantic Segmentation Model",
    "section": "Final Results — integration into solver tool",
    "text": "Final Results — integration into solver tool\nThe enhanced version of the code base I discussed in my prior post can be found here. This version of the model shows some slight activation on background features, which is perhaps the sign of some overfitting.\n\n\n\n\n\nTo conclude, this approach showed some pretty impressive results, especially given the tiny amount of training data that was used!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "👋 Hey!\nI’m Sam. Right now I work in Machine Learning. I also like to write stuff from time to time."
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/3.html",
    "href": "notes/karpathy-zero-to-hero/3.html",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "section": "",
    "text": "Course Page: https://karpathy.ai/zero-to-hero.html"
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/3.html#final-layer-bias-initilization",
    "href": "notes/karpathy-zero-to-hero/3.html#final-layer-bias-initilization",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "section": "1. Final Layer Bias Initilization",
    "text": "1. Final Layer Bias Initilization\nIt can be common to see a “hockey stick” plot when analyzing the loss plots of a neural network for the first few iterations of training. This happens when network is first learning to scale the logits into the same range as the labels.\nTo mitigate this, the bias of the final layer can be set based on the type of ML task, and the balance of the dataset used. For the example of Makemore, Andrej sets the bias to zero to correspond to a uniform distribution. He could have also investigated the distribution of letters in the dataset and set the bias according to that - this is kind of like Empirical Bayes!\nFor other cases, Andrej provides advice in his recipe for training neural nets:\n\n\ninit well. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias."
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/3.html#weight-initialization",
    "href": "notes/karpathy-zero-to-hero/3.html#weight-initialization",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "section": "2. Weight Initialization",
    "text": "2. Weight Initialization\nLarge values in matrices in pre-activation states can lead to strange behaviour when passed through activation functions. For example, activation functions that have asymptotic areas (tanh, sigmoid) can “saturate”, leading to a high number of outputs which are either 0 or 1. This can cause more problems in deep networks. Ideally we want activations in the network to stay within the unit Gaussian range - mean of 0, std of 1\nWe can fix this be initializing weights intelligently. In theory, this can be done manually by scaling down randomly initialized weights, and inspecting the effects on the distribution of activations. However this approach is not scalable to larger networks.\nKaiming (AKA He) Initialization is a more principled way of doing this. It is based on a mathematical analysis of the effects on the statistical changes made by activation functions.\nThis is functionally the same as what Andrej shows in the lecture - dividing the unit normal weights by \\(gain * \\sqrt{n_l}\\), where \\(n_l\\) is the “fan-in”, the number of inputs to the layer, and \\(gain\\) is an activation specific constant. These constants are defined in pytorch too:\n\n\n\n\n\n\n\nnonlinearity\ngain\n\n\n\n\nLinear / Identity\n1\n\n\nConv{1,2,3}D\n1\n\n\nSigmoid\n1\n\n\nTanh\n\\(\\frac{5}{3}\\)​\n\n\nReLU\n\\(\\sqrt{2}\\)​\n\n\nLeaky Relu\n\\(\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}\\)​​\n\n\nSELU\n\\(\\frac{3}{4}\\)\n\n\n\nThe paper first describing this: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - He, K. et al. (2015)\n\nWorked example\nLet’s imagine some neuron inputs x and neuron weights w. We can calculate the matrix product y before passing the resulting values through an activation function RELU to get some activations. When we compare the standard deviation in h to x we can see there has been an increase.\n\nx = torch.randn(1000, 20) # inputs\nw = torch.randn(20, 200) # neuron weights\ny = x @ w\nh = torch.relu(y)\n\n\nprint(x.std(), h.std())\n\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.hist(x.view(-1), density=True);\nplt.subplot(122)\nplt.hist(h.view(-1), density=True);\n\ntensor(0.9962) tensor(2.5542)\n\n\n\n\n\ntorch.randn draws values from a unit normal distribution, \\(\\mu=0, \\sigma=1\\) - so we can change the standard deviation by scaling it by a factor. For RELU, our Kaiming initialization should have \\(\\sigma = \\sqrt{\\frac{2}{n_l}}\\), where \\(n_l\\) is the “fan-in” - the number of inputs to the neuron\n\nw = torch.randn(20, 200) * (2 / 20)**0.5 # init w with He Initialization\ny = x @ w # x is fixed from above\nh = torch.relu(y)\n\nprint(x.std(), h.std())\n\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.hist(x.view(-1), density=True);\nplt.subplot(122)\nplt.hist(h.view(-1), density=True);\n\ntensor(0.9962) tensor(0.8292)\n\n\n\n\n\nWe can see that this has reduced the standard deviation of the activations, closer to the range of unit normal (although obviously the output of RELU is not normally distributed)\n\n\n\n\n\n\nNOTE: Default parameter initialization in torch.nn.Linear\n\n\n\n\n\nWhen taking a look at the source code of nn.Linear, it might seem like they are using the Kaiming init:\ndef reset_parameters(self) -&gt; None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n        # https://github.com/pytorch/pytorch/issues/57109\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0\n            init.uniform_(self.bias, -bound, bound)\nHowever from the comment it becomes clear that actually they are using the nn.init.kaiming_uniform_ function as a hack to achieve values from the uniform distribution \\(U(-1/\\sqrt(in\\_features), 1/\\sqrt(in\\_features))\\)\nTo me it seems strange that they are not using an init defined in nn.init for this…\nDelving into the pytorch forums, it seems that this default init scheme is non standard:\n\n\n\nOne answer in this older thread suggests that the initialisation resembles what is referred to “LeCun Initialisation”. This comment is probably long overdue, but pytorch does not implement LeCun or He/Kaiming initialisation for the Linear module.\n\n\nIf we go through the code (v1.5.0) of Linear.reset_parameters, the first line initialises the weight matrix: init.kaiming_uniform_(self.weight, a=math.sqrt(5)). If we take a look at how kaiming_uniform is implemented, we find that this line is equivalent to\n\nfan = tensor.size(1)  # fan-in for linear, as computed by _calculate_correct_fan\ngain = math.sqrt(2.0 / (1 + a ** 2))  # gain, as computed by calculate_gain\nstd = gain / math.sqrt(fan)\nbound = math.sqrt(3.0) * std\nwith torch.no_grad():\nreturn tensor.uniform_(-bound, bound)\n\n\nSince a = math.sqrt(5) the weights are initialised with std = 1 / math.sqrt(3.0 * fan_in). For reference, LeCun initialisation would be 1 / math.sqrt(fan_in) and He initialisation uses math.sqrt(2 / fan_in).\n\n\nThe bias initialisation in Linear.reset_parameters reveals another problem. Although biases are normally initialised with zeros (for the sake of simplicity), the idea is probably to initialise the biases with std = math.sqrt(1 / fan_in) (cf. LeCun init). By using this value for the boundaries of the uniform distribution, the resulting distribution has std math.sqrt(1 / 3.0 * fan_in), which happens to be the same as the standard deviation for the weights.\n\n\nA more reasonable default for me would be to use LeCun initialisation (since this has been the go-to standard since 1998). I could also understand Kaiming initialisation as the default, because everyone is using ReLU activation functions everywhere anyway (although I have a feeling that this is not necessarily the case for people working with fully connected networks). Some time ago, I submitted a pull request to adopt LeCun initialisation as the default, but there seems to be little incentive to actually make changes due to backward compatibility.\n\n\nThis probably also explains why pytorch ended up with its own initialisation strategy for fully connected networks. Someone must have forgotten about the fact that a uniform distribution with bounds -b, b has a standard deviation of b / math.sqrt(3) instead of just b. Due to backwards compatibility this got stuck and no-one is willing to make the change to the more widely accepted and standard initialisation."
  },
  {
    "objectID": "notes/karpathy-zero-to-hero/3.html#bias-initialization",
    "href": "notes/karpathy-zero-to-hero/3.html#bias-initialization",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "section": "Bias Initialization",
    "text": "Bias Initialization\nFor biases not in the final layer, both Kaiming Initialization and other regimes set the bias to 0"
  }
]