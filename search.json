[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog_new",
    "section": "",
    "text": "üö¢ Debugging for Dockerized ML applications in Python\n\n\n\n\n\n\n\nml\n\n\npython\n\n\n\n\nUsing VScode and debugpy to make debugging a breeze\n\n\n\n\n\n\nSam Watts\n\n\n\n\n\n\n  \n\n\n\n\nRecSys 2022 - My Top 5 Papers\n\n\n\n\n\n\n\nml\n\n\nrecsys\n\n\n\n\nHighlights from the conference\n\n\n\n\n\n\nSam Watts\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGoogle Timeline: The Big Brother of Your Transport Emissions\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-07-31-debugging-dockerized-ml-python.html",
    "href": "posts/2021-07-31-debugging-dockerized-ml-python.html",
    "title": "üö¢ Debugging for Dockerized ML applications in Python",
    "section": "",
    "text": "Docker has become ubiquitous in ML applications in the last few years. It can enable easy collaboration between engineers with different hardware ‚Äî as well as easing the transition from prototyping on personal laptops to compute clusters in production. On the flip side, it introduces an extra layer of complexity for engineers to work with when developing and maintaining productionised models.\nIn my professional work, I‚Äôve found debugging to be one of the things made harder by this extra layer of complexity. In this post I‚Äôm going to outline my current setup with VSCode and debugpy that greatly simplifies this process when applied to a model training application."
  },
  {
    "objectID": "posts/2021-07-31-debugging-dockerized-ml-python.html#why-do-we-need-this",
    "href": "posts/2021-07-31-debugging-dockerized-ml-python.html#why-do-we-need-this",
    "title": "üö¢ Debugging for Dockerized ML applications in Python",
    "section": "Why do we need this?",
    "text": "Why do we need this?\nWhen debugging code, we want to be able to inspect its environment at runtime as accurately as possible. Any deviation from this can lead to fixes that plainly don‚Äôt work in the runtime environment.\nWhilst getting to grips with Docker, my debugging process would generally entail recreating the scaffolding around the Python script that I wanted to inspect in my local development environment, and then debugging that script directly. In model training applications that contain bash scripts, multiple entrypoints and use of environment variables, this can quickly add a large development overhead. With this added complexity comes the increased chance of errors creeping in, rendering the whole process slow and frustrating.\nSo how do we avoid this? We need a debugging system that can interact with Docker, and let our code run as it was designed to!"
  },
  {
    "objectID": "posts/2021-07-31-debugging-dockerized-ml-python.html#the-solution",
    "href": "posts/2021-07-31-debugging-dockerized-ml-python.html#the-solution",
    "title": "üö¢ Debugging for Dockerized ML applications in Python",
    "section": "The Solution",
    "text": "The Solution\nWhat worked best in the end for me is a debugger that can connect to a Docker container where your model training application is running, and directly inspect the environment of a given Python script.\nThis is where debugpy comes in!\nPreviously known as ptvsd, this package is developed by Microsoft specifically for use in VSCode with Python. It implements all of the common debugging tools you would expect, as well as allowing for attaching to remote environments, such as Docker containers or even remote machines via SSH.\nAs an aside, debugpy implements the Debug Adapter Protocol (DAP), which is a standardised way for development tools to communicate with debuggers.\nUsing debugpy with Docker containers is wonderfully simple, and requires 3 distinct steps. I‚Äôll dive into each of these in turn, before demonstrating the whole process afterwards.\n\nConfiguring debugpy in Python\nConfiguring the connection to the Docker container\nSetting up breakpoints\n\n\n1. Configuring debugpy in Python\nIn the script you would like to debug, the following snippet should be added before any other code.\nimport debugpy\n\ndebugpy.listen((\"0.0.0.0\", 5678))\nprint(\"Waiting for client to attach...\")\ndebugpy.wait_for_client()\nThis will setup debugpy to listen on port 5678 for a client to attach, and will also pause the execution until a client connects via that port.\n\n\n2. Configuring the connection to the Docker container\nNow we have our Python script configured, we need to make sure the VSCode debugger client can connect to debugpy when it is running inside a Docker container.\nFirstly, when you run your Docker container, the port that debugpy is listening on must be mapped to a local port\ndocker run \\\n    -p 5678:5678 \\  # map container port to local port\n    temp-container\nSecondly, we need to create a launch.json file to configure how the local VSCode debugging client will run. This minimal example tells the debugger to attach to port 5678, which will be mapped to the Docker port of the same number when we run the container.\n{\n   \"version\":\"0.2.0\",\n   \"configurations\":[\n      {\n         \"name\":\"Python: Docker Attach\",\n         \"type\":\"python\",\n         \"request\":\"attach\",\n         \"connect\":{\n            \"host\":\"localhost\",\n            \"port\":5678\n         },\n         \"pathMappings\":[\n            {\n               \"localRoot\":\"${workspaceFolder}\",\n               \"remoteRoot\":\".\"\n            }\n         ]\n      }\n   ]\n}\n\n\n3. Setting up breakpoints\nI was surprised when I first tried this that when you set breakpoints via the VSCode UI on the local version of a Python script, that will correspond to the copied scripts that run inside your Docker container! Pure wizardry from VSCode.\nIn addition, you can also use debugpy.breakpoint() to explicitly set breakpoints via the debugpy API. An additional benefit of this is that these calls will be ignored if you exclude the debugpy configuration mentioned in step (1), providing a quick way of temporarily removing debugging."
  },
  {
    "objectID": "posts/2021-07-31-debugging-dockerized-ml-python.html#debugging-in-action",
    "href": "posts/2021-07-31-debugging-dockerized-ml-python.html#debugging-in-action",
    "title": "üö¢ Debugging for Dockerized ML applications in Python",
    "section": "Debugging in action",
    "text": "Debugging in action\nYou should be good to go! The steps to debug are:\n\nAdd breakpoints in the UI\nRebuild and run the Docker container\nConnect the debugger\n\n\nThe full code used for this example can be found below. Happy debugging! üòÉ\nGitHub - sam-watts/vscode-docker-debugging: A template for debugging long running, dockerized programs in python with vscode"
  },
  {
    "objectID": "posts/2021-07-31-debugging-dockerized-ml-python.html#links",
    "href": "posts/2021-07-31-debugging-dockerized-ml-python.html#links",
    "title": "üö¢ Debugging for Dockerized ML applications in Python",
    "section": "Links",
    "text": "Links\n\ndebugpy Github: https://github.com/microsoft/debugpy\nDocker run reference: https://docs.docker.com/engine/reference/run/"
  },
  {
    "objectID": "posts/2022-10-10-recys-2022-top-5.html",
    "href": "posts/2022-10-10-recys-2022-top-5.html",
    "title": "RecSys 2022 - My Top 5 Papers",
    "section": "",
    "text": "After reading papers from RecSys for several years, I was really happy to be able to (virtually) attend for the first time this year. If you haven‚Äôt heard of it, RecSys is the most important conference for new results in the field of recommender systems research.\nThis was also my first academic conference of any kind! I found the mix of academic and industry talks really balanced each other out well‚Ää-‚Ääit was great to see exciting theory-driven ideas alongside real world implementation stories, with all the engineering problems that come with them.\nBelow are the papers I found most interesting from the main conference, in no particular order. Links to the papers PDFs are included in the sub-titles.\n\nAugmenting Netflix Search with In-Session Adapted Recommendations\n\nBased on user research, Netflix found that users typically fall into 3 categories when searching for content: * Fetch‚Ää - ‚Ääthe user knows exactly what they want, and generally enter a query for an exact film or TV show * Find‚Ää - ‚Ääthe user broadly knows the kind of thing they want to see, but it‚Äôs not a fully formed idea‚Ää-‚Ääeg. comedy movies * Explore‚Ää - ‚Ääthe user has no fixed idea of what they are looking for, and are open to suggestions\nBased on this, Netflix reasoned that there was an opportunity to present the ‚ÄúExplore‚Äù users with recommendations on the pre-search page. The key point is that these recommendations would need to take into account user interactions from the current session, to align with whatever they might be looking for at the current moment. These kind of recommendations are commonly referred to as ‚ÄúAnticipatory Search‚Äù or ‚ÄúPre-search Recommendations‚Äù.\nThe authors designed a model to provide recommendations for this use case. This model uses features such as historical user data, context about the user and the session, and well as raw sequences of in-session browsing interactions. Video metadata and embeddings are also used to provide information about the items that are interacted with.\n\nThe authors experimented with different types of deep architectures, including both dense and sparse features. This was coupled with the raw interaction sequence, which they modelled with different types of neural network modules that can accommodate sequence data‚Ää-‚Ääattention blocks, LSTM and GRU.\n\nThey pick out a specific example of how the model reacts to in-session browsing activity. The ranking of the recommendations is influenced by the titles browsed by the user, which they contend should result in a good experience for the user when they navigate to the search page.\n\nNo specifics are mentioned regarding which objectives are used to train the model, or how the model performs when tested online. In offline performance they see a 6% relative increase in ranking metrics against the current production model. I would be interested in hearing more about the cost-benefit tradeoff involved in deploying this model online, due to the large engineering challenge required to make real-time features available to the model at inference time.\n\n\nRecommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)\nThis paper borrows heavily from recent advances in NLP models to create a multi-purpose model for different recommendation tasks.\n\nP5 is directly inspired by the influential T5 paper. T5 applies a unified approach to transfer learning, to effectively learn multiple tasks as part of a text-to-text framework. P5 applies these concepts to recommendation tasks, as shown below.\n\nThe P5 framework allows for the use of prompt templates, which are filled with user interaction data and item metadata, to produce a fully text-based input based on the task described in the prompt. This means that all tasks can be learned simultaneously during pre-training. The authors present very promising results for this architecture compared to other state of the art models. If they can be replicated, this could lead to the use of more multi-purpose, pre-trained models in industry. I for one, look forward to becoming a rockstar prompt engineer! üöÄ\n\n\nTowards Psychologically-Grounded Dynamic Preference Models\nOne of the core assumptions in many recommender models is that user preferences are static. But what if a user‚Äôs preferences change due to the items we are showing the user with our recommender system? How could this feedback loop effect what a user wants over time? This paper focuses on a framework for formalising possible user preference changes due to human psychological effects.\nThe best example included is the ‚ÄúMere Exposure Effect‚Äù‚Ää-‚Ääwhich states that people are more likely to develop a preference for something that they are familiar with. This was first described by Robert Zajonc in the 60s‚Ää-‚Äähis experiment included nonsense words on the cover of a student newspaper. When tested, on average the students who read this paper rated the words they had been exposed to as more positive-sounding compared to other nonsense words.\nHow better to explain this all, than a graph with a grumpy cat? üòª\n\nThe authors formalise the mere exposure effect applied to recommendations mathematically, as show on the left. A user‚Äôs initial preferences (œÄ‚Çú) and items (ŒΩ) are both represented as vectors. In response to being recommended an item ŒΩ at time step t+1, the users preference moves from their starting preference vector, along the line the intersects the user‚Äôs baseline preference and the item vectors. This results in the updated preference vector, œÄ‚Çú‚Çä‚ÇÅ. The factor Œ≥ controls how far along this line the preferences move (where Œ≥ œµ [0, 1]). The graph below the equation depicts this in a 2-dimensional preference space.\nThe authors propose similar formulations for Operant Conditioning and Hedonic Adaptation, before including a section on simulations based on these ideas. This includes discussion on how recommenders may achieve different engagement scores in the case where user preferences are dynamic.\nThe ideas in this paper feel some way from making it into most industrial settings anytime soon‚Ää-‚Ääbut a more holistic focus on the role platforms might be playing in moulding user preferences is definitely welcome.\n\n\nReusable Self-Attention Recommender Systems in Fashion Industry Applications\nContinuing the trend of unifying models‚Ää-‚Äähere engineers at the fashion website Zalando present their work on creating a single recommender model architecture that can be reused for several tasks, using the now ever-present Transformer architecture.\n\nThe authors unified the training datasets previously fed into separate models, and used them to train a recommender architecture that can be re-used for 3 different tasks: outfit ranking, outfit recommendation, real-time and personalised outfit generation. For the different subtasks, small changes are made to the Transformer architecture, and boolean masking is used to hide labels not relevant to the use case the model is being trained for.\nThe inclusion of contextual data also allows the model to work with semi-cold start users, who may have fewer significant interactions with items, as well as fully-cold start users with no item interactions, who can be predicted based on contextual data alone.\nThe model is able to learn from different interaction types due to a one-hot encoding of the interaction type. In addition, a simple integer of days-since-interaction allows the model to balance long and short term interests.¬†\nIn A/B testing the authors report increases in user engagement compared to the previous deployed algorithms of between 5‚Äì130%.\n\n\nStreaming Session-Based Recommendation: When Graph Neural Networks meet the Neighborhood\nI found this paper intriguing for the insight it gives into some methodology issues present in the recommender system literature.¬†\nA recent focus of the RecSys field has been session-based recommendations‚Ää-‚Ääproviding recommendations to users using in-session signals. Several deep learning model architectures have been proposed to address this task.¬†\nThe authors of this papers compared one of these complex approaches against some more simple baselines: * VSKNN+‚Ää-‚Ääa session-based nearest neighbour approach. Finds past sessions that are similar to the current session. Items that appear in these similar sessions are used as recommendation candidates. The authors add an extension to the base VSKNN algorithm, that considers each user‚Äôs past sessions * SR+‚Ää-‚Ää‚ÄúSequential Rules‚Äù, a variation of association rule learning. A rule is created when an item p appears after item q in a session, where the weight of the rule is a function of the number of items between p and q in the interaction sequence. The authors again extend this method to consider each user‚Äôs past sessions * GAG‚Ää-‚Ääan approached based on a Graph Neural Network\n\nThe final findings are shown in the table above, are that a hybrid approach (which simply combines the recommended items produced by VSKNN+ and SR+) performs better than GAG across all metrics and datasets the authors tested. Neither of these baselines were used as comparisons in the original paper that proposed GAG.\n\nIn all, it was a great conference with so many intelligent people to learn from‚Ää-‚ÄäI would really recommend it! I hope you found this interesting üëã"
  },
  {
    "objectID": "posts/2023-03-01-google-timeline-transport-emissions.html",
    "href": "posts/2023-03-01-google-timeline-transport-emissions.html",
    "title": "blog_new",
    "section": "",
    "text": "Main point - I‚Äôm going flight free in 2023\nRecently I came across this image whilst browsing instagram (their audience targeting is clearly pretty good‚Ä¶)\n\n\n\nTaken from Flight Free UK\n\n\nI‚Äôve been taking trains through europe for a while now - since moving to Denmark a year and a half ago, it‚Äôs become my main method of transport for visiting the UK. I generally spread the trains over 2 days, with an overnight in Germany. Obviously the cost is much higher than flying,\n\nRoyal Soceity - to meet current levels of aviation we would need one of:\n\nHalf of all agricultural land in UK to be used for biofuel\nDouble the current renewable grid capacity for hydrogen production\n\nThis becomes even more challenging when you consider the increased demand on the electricity grid from personal vehicle and home heating electrification, which in the UK is projected to contribute to a 15% increase in demand by 2035\n\n\n\nhttps://www.theguardian.com/business/2023/feb/28/scientists-uk-aviation-net-zero-ambitions-half-farmland-double-renewable-electricity\nOur World in Data - CO2 emissions https://ourworldindata.org/co2-emissions\n\nFlying is 2.5% of CO2 emissions (3.5% of heating when we consider other factors). This makes the problem seem small compared to other sectors.\nOnly 20% of the world population have ever been on a plane - this 3.5% is concentrated amongst a minority of the population\nThis seem like a class problem. ie. if you have the finanancial ability to get on plane, then you could be making far better choices.\nComparison - in typical people‚Äôs lifestyles, what % does flying contribution towards their carbon footprint?\nReductions towards net zero are fine if we anticipate that every other country is trying to reach net zero at the same time.\nWhen we look at other countries, this becomes problematic\nOur quality of living in europe is contingent on an outsized amount of historic emitted co2\n1% of global population are responsible for 50% of aviation emissions\n\n\nThe frequent flyers identified in the study travelled about 35,000 miles (56,000km) a year, G√∂ssling said, equivalent to three long-haul flights a year, one short-haul flight per month, or some combination of the two.\n\nhttps://www.theguardian.com/business/2020/nov/17/people-cause-global-aviation-emissions-study-covid-19\nBeing vegan for a year can save around 1 tonne of CO2 per year - which can be eliminated by taking a single flight!"
  },
  {
    "objectID": "posts/2023-03-14-draft.html",
    "href": "posts/2023-03-14-draft.html",
    "title": "Google Timeline: The Big Brother of Your Transport Emissions",
    "section": "",
    "text": "A few weeks ago I came across this image and found the message striking. I‚Äôve followed a vegan diet for the past 5 years, and try my best to take the train when travelling across europe, so I have some awareness of these impacts. Even so, the magnitude was surprising.\n\n\n\nTaken from Flight Free UK\n\n\nI wanted to see how much of my own transport emissions were coming from flights, and how that stacks up against the average person.\nBeing a lazy programmer, I decided to try and use Google Timeline data to get this rather than manually entering all my flights for the last n years. If you are have Google Timeline enabled in the Google Maps app, you can download your data from Google Takeout. You can take a look at own emissions by visiting the app I created to visualize this data: https://transport-co2-emissions.streamlit.app/\n\n<p>The results for me personally? It‚Äôs not a pretty picture. In 2017, the worst year I have data for, I took 13 flights</p>\n<p>For starters, flying is distributed highly non-uniformly across the global population. An often quoted statistic is that 1% of the world‚Äôs population accounts for 50% of all flights.</p>\n<p>More worryingly, the path towards decarbonization for aviation is unclear. In general, airlines seem to be relying on a combination of sustainable aviation fuel (SAFs) and carbon offsets to reach net zero. Just a week ago the Royal Society published a report into the viability of SAFs in covering the current level of flying in the UK aviation industry, with two main conclusions: 1) To use biofuels, half of all agricultural land in UK would need to be repurposed 2) To use hydrogen, double the current renewable energy production level would be needed</p>\n<p>This becomes even more challenging when you consider the increased demand on the electricity grid from personal vehicle and home heating electrification, which in the UK is projected to contribute to a 15% increase in demand by 2035.</p>\n<p>As it stands, the continuing</p>\n<p>Since diving into these numbers, I decided to take Flight Free UK‚Äôs challenge of a flight free year for 2023.</p>\n<div id=\"quarto-navigation-envelope\" class=\"hidden\">\n<p><span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\">blog_new</span> <span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\">blog_new</span> <span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\">About</span></p>\n</div>\n<div id=\"quarto-meta-markdown\" class=\"hidden\">\n<p><span class=\"hidden\" data-render-id=\"quarto-metatitle\">blog_new - Google Timeline: The Big Brother of Your Transport Emissions</span> <span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\">blog_new - Google Timeline: The Big Brother of Your Transport Emissions</span> <span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\">blog_new - Google Timeline: The Big Brother of Your Transport Emissions</span> <span class=\"hidden\" data-render-id=\"quarto-metasitename\">blog_new</span></p>\n</div>\n\n</main> <!-- /main -->\n<script id = \"quarto-html-after-body\" type=\"application/javascript\">\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) => {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () => {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"Óßã\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    target: function(trigger) {\n      return trigger.previousElementSibling;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i<noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n  const findCites = (el) => {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i<bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n</script>\n</div> <!-- /content -->\n\n</body>\n\n</html>"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]