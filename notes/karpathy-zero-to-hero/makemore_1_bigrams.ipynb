{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = sorted(list(set(\"\".join(names))))\n",
    "letters.append(\".\")\n",
    "ltoi = {l:i for i, l in enumerate(letters)}\n",
    "itol = {i:l for l, i in ltoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "xs_bigram, ys_bigram = [], []\n",
    "for n in names:\n",
    "    n = \".\" + n + \".\"\n",
    "    for first, second in zip(n, n[1:]):\n",
    "        xs_bigram.append(ltoi[first])\n",
    "        ys_bigram.append(ltoi[second])\n",
    "        \n",
    "xs_bigram = torch.tensor(xs_bigram)\n",
    "ys_bigram = torch.tensor(ys_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "inputs = torch.nn.functional.one_hot(xs_bigram).float()\n",
    "logits = inputs @ W \n",
    "probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(ys_bigram.shape[0]), ys_bigram].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8877, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates\n",
    "W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(3.7675, grad_fn=<NegBackward0>)\n",
      "1 tensor(3.3568, grad_fn=<NegBackward0>)\n",
      "2 tensor(3.1371, grad_fn=<NegBackward0>)\n",
      "3 tensor(3.0079, grad_fn=<NegBackward0>)\n",
      "4 tensor(2.9158, grad_fn=<NegBackward0>)\n",
      "5 tensor(2.8464, grad_fn=<NegBackward0>)\n",
      "6 tensor(2.7934, grad_fn=<NegBackward0>)\n",
      "7 tensor(2.7524, grad_fn=<NegBackward0>)\n",
      "8 tensor(2.7200, grad_fn=<NegBackward0>)\n",
      "9 tensor(2.6940, grad_fn=<NegBackward0>)\n",
      "10 tensor(2.6726, grad_fn=<NegBackward0>)\n",
      "11 tensor(2.6547, grad_fn=<NegBackward0>)\n",
      "12 tensor(2.6395, grad_fn=<NegBackward0>)\n",
      "13 tensor(2.6263, grad_fn=<NegBackward0>)\n",
      "14 tensor(2.6148, grad_fn=<NegBackward0>)\n",
      "15 tensor(2.6046, grad_fn=<NegBackward0>)\n",
      "16 tensor(2.5956, grad_fn=<NegBackward0>)\n",
      "17 tensor(2.5876, grad_fn=<NegBackward0>)\n",
      "18 tensor(2.5805, grad_fn=<NegBackward0>)\n",
      "19 tensor(2.5740, grad_fn=<NegBackward0>)\n",
      "20 tensor(2.5682, grad_fn=<NegBackward0>)\n",
      "21 tensor(2.5629, grad_fn=<NegBackward0>)\n",
      "22 tensor(2.5581, grad_fn=<NegBackward0>)\n",
      "23 tensor(2.5536, grad_fn=<NegBackward0>)\n",
      "24 tensor(2.5495, grad_fn=<NegBackward0>)\n",
      "25 tensor(2.5457, grad_fn=<NegBackward0>)\n",
      "26 tensor(2.5421, grad_fn=<NegBackward0>)\n",
      "27 tensor(2.5388, grad_fn=<NegBackward0>)\n",
      "28 tensor(2.5357, grad_fn=<NegBackward0>)\n",
      "29 tensor(2.5328, grad_fn=<NegBackward0>)\n",
      "30 tensor(2.5301, grad_fn=<NegBackward0>)\n",
      "31 tensor(2.5275, grad_fn=<NegBackward0>)\n",
      "32 tensor(2.5251, grad_fn=<NegBackward0>)\n",
      "33 tensor(2.5228, grad_fn=<NegBackward0>)\n",
      "34 tensor(2.5206, grad_fn=<NegBackward0>)\n",
      "35 tensor(2.5185, grad_fn=<NegBackward0>)\n",
      "36 tensor(2.5166, grad_fn=<NegBackward0>)\n",
      "37 tensor(2.5147, grad_fn=<NegBackward0>)\n",
      "38 tensor(2.5129, grad_fn=<NegBackward0>)\n",
      "39 tensor(2.5113, grad_fn=<NegBackward0>)\n",
      "40 tensor(2.5097, grad_fn=<NegBackward0>)\n",
      "41 tensor(2.5081, grad_fn=<NegBackward0>)\n",
      "42 tensor(2.5067, grad_fn=<NegBackward0>)\n",
      "43 tensor(2.5053, grad_fn=<NegBackward0>)\n",
      "44 tensor(2.5040, grad_fn=<NegBackward0>)\n",
      "45 tensor(2.5027, grad_fn=<NegBackward0>)\n",
      "46 tensor(2.5015, grad_fn=<NegBackward0>)\n",
      "47 tensor(2.5003, grad_fn=<NegBackward0>)\n",
      "48 tensor(2.4992, grad_fn=<NegBackward0>)\n",
      "49 tensor(2.4981, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# init and training for bigram model\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "for i in range(50):\n",
    "    # forward\n",
    "    inputs = torch.nn.functional.one_hot(xs_bigram).float()\n",
    "    logits = inputs @ W \n",
    "    probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(ys_bigram.shape[0]), ys_bigram].log().mean()\n",
    "    \n",
    "    print(i, loss)\n",
    "    \n",
    "    # backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # updates\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "E01: train a trigram language model, i.e. take two characters as an input to\n",
    "predict the 3rd one. Feel free to use either counting or a neural net. Evaluate\n",
    "the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "xs_trigram, ys_trigram = [], []\n",
    "for n in names:\n",
    "    n = \".\" * 2 + n + \".\"\n",
    "    for first, second, third in zip(n, n[1:], n[2:]):\n",
    "        # print(first, second, third)\n",
    "        xs_trigram.append([ltoi[first], ltoi[second]])\n",
    "        ys_trigram.append(ltoi[third])\n",
    "        \n",
    "xs_trigram = torch.tensor(xs_trigram)\n",
    "ys_trigram = torch.tensor(ys_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.nn.functional.one_hot(xs_trigram).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(4.1870, grad_fn=<NegBackward0>)\n",
      "1 tensor(3.7626, grad_fn=<NegBackward0>)\n",
      "2 tensor(3.5288, grad_fn=<NegBackward0>)\n",
      "3 tensor(3.3593, grad_fn=<NegBackward0>)\n",
      "4 tensor(3.2326, grad_fn=<NegBackward0>)\n",
      "5 tensor(3.1353, grad_fn=<NegBackward0>)\n",
      "6 tensor(3.0582, grad_fn=<NegBackward0>)\n",
      "7 tensor(2.9955, grad_fn=<NegBackward0>)\n",
      "8 tensor(2.9438, grad_fn=<NegBackward0>)\n",
      "9 tensor(2.9003, grad_fn=<NegBackward0>)\n",
      "10 tensor(2.8631, grad_fn=<NegBackward0>)\n",
      "11 tensor(2.8310, grad_fn=<NegBackward0>)\n",
      "12 tensor(2.8029, grad_fn=<NegBackward0>)\n",
      "13 tensor(2.7780, grad_fn=<NegBackward0>)\n",
      "14 tensor(2.7558, grad_fn=<NegBackward0>)\n",
      "15 tensor(2.7358, grad_fn=<NegBackward0>)\n",
      "16 tensor(2.7176, grad_fn=<NegBackward0>)\n",
      "17 tensor(2.7011, grad_fn=<NegBackward0>)\n",
      "18 tensor(2.6859, grad_fn=<NegBackward0>)\n",
      "19 tensor(2.6719, grad_fn=<NegBackward0>)\n",
      "20 tensor(2.6590, grad_fn=<NegBackward0>)\n",
      "21 tensor(2.6470, grad_fn=<NegBackward0>)\n",
      "22 tensor(2.6358, grad_fn=<NegBackward0>)\n",
      "23 tensor(2.6253, grad_fn=<NegBackward0>)\n",
      "24 tensor(2.6156, grad_fn=<NegBackward0>)\n",
      "25 tensor(2.6064, grad_fn=<NegBackward0>)\n",
      "26 tensor(2.5977, grad_fn=<NegBackward0>)\n",
      "27 tensor(2.5896, grad_fn=<NegBackward0>)\n",
      "28 tensor(2.5819, grad_fn=<NegBackward0>)\n",
      "29 tensor(2.5746, grad_fn=<NegBackward0>)\n",
      "30 tensor(2.5677, grad_fn=<NegBackward0>)\n",
      "31 tensor(2.5611, grad_fn=<NegBackward0>)\n",
      "32 tensor(2.5549, grad_fn=<NegBackward0>)\n",
      "33 tensor(2.5490, grad_fn=<NegBackward0>)\n",
      "34 tensor(2.5433, grad_fn=<NegBackward0>)\n",
      "35 tensor(2.5379, grad_fn=<NegBackward0>)\n",
      "36 tensor(2.5328, grad_fn=<NegBackward0>)\n",
      "37 tensor(2.5278, grad_fn=<NegBackward0>)\n",
      "38 tensor(2.5231, grad_fn=<NegBackward0>)\n",
      "39 tensor(2.5186, grad_fn=<NegBackward0>)\n",
      "40 tensor(2.5143, grad_fn=<NegBackward0>)\n",
      "41 tensor(2.5101, grad_fn=<NegBackward0>)\n",
      "42 tensor(2.5061, grad_fn=<NegBackward0>)\n",
      "43 tensor(2.5023, grad_fn=<NegBackward0>)\n",
      "44 tensor(2.4986, grad_fn=<NegBackward0>)\n",
      "45 tensor(2.4951, grad_fn=<NegBackward0>)\n",
      "46 tensor(2.4917, grad_fn=<NegBackward0>)\n",
      "47 tensor(2.4884, grad_fn=<NegBackward0>)\n",
      "48 tensor(2.4852, grad_fn=<NegBackward0>)\n",
      "49 tensor(2.4821, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W1 = torch.randn((27, 27), requires_grad=True)\n",
    "W2 = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "for i in range(50):\n",
    "    logits = inputs[:, 0, :] @ W1 + inputs[:, 1, :] @ W2\n",
    "\n",
    "    probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(ys_trigram.shape[0]), ys_trigram].log().mean()\n",
    "\n",
    "    print(i, loss)\n",
    "    \n",
    "    # backward\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # updates\n",
    "    W1.data += -20 * W1.grad\n",
    "    W2.data += -20 * W2.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewritten as a class below - training looks to be equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn((27, 27)))\n",
    "        self.w2 = nn.Parameter(torch.randn((27, 27)))\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        inputs = torch.nn.functional.one_hot(x).float()\n",
    "        logits = inputs[:, 0, :] @ self.w1 + inputs[:, 1, :] @ self.w2\n",
    "        probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = -probs[torch.arange(targets.shape[0]), targets].log().mean()\n",
    "\n",
    "        return probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(4.3345, grad_fn=<NegBackward0>)\n",
      "1 tensor(3.9514, grad_fn=<NegBackward0>)\n",
      "2 tensor(3.6622, grad_fn=<NegBackward0>)\n",
      "3 tensor(3.4633, grad_fn=<NegBackward0>)\n",
      "4 tensor(3.3159, grad_fn=<NegBackward0>)\n",
      "5 tensor(3.1994, grad_fn=<NegBackward0>)\n",
      "6 tensor(3.1078, grad_fn=<NegBackward0>)\n",
      "7 tensor(3.0346, grad_fn=<NegBackward0>)\n",
      "8 tensor(2.9750, grad_fn=<NegBackward0>)\n",
      "9 tensor(2.9253, grad_fn=<NegBackward0>)\n",
      "10 tensor(2.8830, grad_fn=<NegBackward0>)\n",
      "11 tensor(2.8464, grad_fn=<NegBackward0>)\n",
      "12 tensor(2.8145, grad_fn=<NegBackward0>)\n",
      "13 tensor(2.7862, grad_fn=<NegBackward0>)\n",
      "14 tensor(2.7610, grad_fn=<NegBackward0>)\n",
      "15 tensor(2.7385, grad_fn=<NegBackward0>)\n",
      "16 tensor(2.7181, grad_fn=<NegBackward0>)\n",
      "17 tensor(2.6997, grad_fn=<NegBackward0>)\n",
      "18 tensor(2.6830, grad_fn=<NegBackward0>)\n",
      "19 tensor(2.6677, grad_fn=<NegBackward0>)\n",
      "20 tensor(2.6538, grad_fn=<NegBackward0>)\n",
      "21 tensor(2.6410, grad_fn=<NegBackward0>)\n",
      "22 tensor(2.6292, grad_fn=<NegBackward0>)\n",
      "23 tensor(2.6183, grad_fn=<NegBackward0>)\n",
      "24 tensor(2.6083, grad_fn=<NegBackward0>)\n",
      "25 tensor(2.5989, grad_fn=<NegBackward0>)\n",
      "26 tensor(2.5902, grad_fn=<NegBackward0>)\n",
      "27 tensor(2.5821, grad_fn=<NegBackward0>)\n",
      "28 tensor(2.5745, grad_fn=<NegBackward0>)\n",
      "29 tensor(2.5674, grad_fn=<NegBackward0>)\n",
      "30 tensor(2.5608, grad_fn=<NegBackward0>)\n",
      "31 tensor(2.5545, grad_fn=<NegBackward0>)\n",
      "32 tensor(2.5485, grad_fn=<NegBackward0>)\n",
      "33 tensor(2.5429, grad_fn=<NegBackward0>)\n",
      "34 tensor(2.5376, grad_fn=<NegBackward0>)\n",
      "35 tensor(2.5326, grad_fn=<NegBackward0>)\n",
      "36 tensor(2.5278, grad_fn=<NegBackward0>)\n",
      "37 tensor(2.5232, grad_fn=<NegBackward0>)\n",
      "38 tensor(2.5188, grad_fn=<NegBackward0>)\n",
      "39 tensor(2.5147, grad_fn=<NegBackward0>)\n",
      "40 tensor(2.5107, grad_fn=<NegBackward0>)\n",
      "41 tensor(2.5069, grad_fn=<NegBackward0>)\n",
      "42 tensor(2.5032, grad_fn=<NegBackward0>)\n",
      "43 tensor(2.4997, grad_fn=<NegBackward0>)\n",
      "44 tensor(2.4963, grad_fn=<NegBackward0>)\n",
      "45 tensor(2.4931, grad_fn=<NegBackward0>)\n",
      "46 tensor(2.4900, grad_fn=<NegBackward0>)\n",
      "47 tensor(2.4870, grad_fn=<NegBackward0>)\n",
      "48 tensor(2.4841, grad_fn=<NegBackward0>)\n",
      "49 tensor(2.4813, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    probs, loss = m.forward(xs_trigram, targets=ys_trigram)\n",
    "    \n",
    "    print(i, loss)\n",
    "    m.zero_grad()\n",
    "    loss.backward()\n",
    "    # updates\n",
    "    m.w1.data += -20 * m.w1.grad\n",
    "    m.w2.data += -20 * m.w2.grad\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test\n",
    "set. Train the bigram and trigram models only on the training set. Evaluate them\n",
    "on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(array):\n",
    "    frac = array.shape[0] * 0.1 // 1\n",
    "    train = array[:int(frac * 8)]\n",
    "    val = array[int(frac * 8): int(frac * 9)]\n",
    "    test = array[int(frac * 9):]\n",
    "    assert train.shape[0] + test.shape[0] + val.shape[0] == array.shape[0]\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs_bigram, val_xs_bigram, test_xs_bigram = split_data(xs_bigram)\n",
    "train_ys_bigram, val_ys_bigram, test_ys_bigram = split_data(ys_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss.data.item()=3.7033, val_loss.data.item()=3.6566, test_loss.data.item()=3.6431\n",
      "1 loss.data.item()=3.2771, val_loss.data.item()=3.3136, test_loss.data.item()=3.3068\n",
      "2 loss.data.item()=3.0436, val_loss.data.item()=3.1405, test_loss.data.item()=3.1388\n",
      "3 loss.data.item()=2.9152, val_loss.data.item()=3.0434, test_loss.data.item()=3.0454\n",
      "4 loss.data.item()=2.8324, val_loss.data.item()=2.9776, test_loss.data.item()=2.9817\n",
      "5 loss.data.item()=2.7750, val_loss.data.item()=2.9334, test_loss.data.item()=2.9378\n",
      "6 loss.data.item()=2.7319, val_loss.data.item()=2.8967, test_loss.data.item()=2.9011\n",
      "7 loss.data.item()=2.6978, val_loss.data.item()=2.8670, test_loss.data.item()=2.8714\n",
      "8 loss.data.item()=2.6699, val_loss.data.item()=2.8417, test_loss.data.item()=2.8460\n",
      "9 loss.data.item()=2.6468, val_loss.data.item()=2.8203, test_loss.data.item()=2.8246\n",
      "10 loss.data.item()=2.6274, val_loss.data.item()=2.8018, test_loss.data.item()=2.8060\n",
      "11 loss.data.item()=2.6109, val_loss.data.item()=2.7859, test_loss.data.item()=2.7901\n",
      "12 loss.data.item()=2.5969, val_loss.data.item()=2.7721, test_loss.data.item()=2.7763\n",
      "13 loss.data.item()=2.5848, val_loss.data.item()=2.7601, test_loss.data.item()=2.7642\n",
      "14 loss.data.item()=2.5743, val_loss.data.item()=2.7496, test_loss.data.item()=2.7536\n",
      "15 loss.data.item()=2.5651, val_loss.data.item()=2.7404, test_loss.data.item()=2.7443\n",
      "16 loss.data.item()=2.5570, val_loss.data.item()=2.7321, test_loss.data.item()=2.7361\n",
      "17 loss.data.item()=2.5498, val_loss.data.item()=2.7248, test_loss.data.item()=2.7287\n",
      "18 loss.data.item()=2.5434, val_loss.data.item()=2.7182, test_loss.data.item()=2.7221\n",
      "19 loss.data.item()=2.5376, val_loss.data.item()=2.7123, test_loss.data.item()=2.7161\n",
      "20 loss.data.item()=2.5324, val_loss.data.item()=2.7069, test_loss.data.item()=2.7107\n",
      "21 loss.data.item()=2.5277, val_loss.data.item()=2.7019, test_loss.data.item()=2.7058\n",
      "22 loss.data.item()=2.5233, val_loss.data.item()=2.6974, test_loss.data.item()=2.7013\n",
      "23 loss.data.item()=2.5193, val_loss.data.item()=2.6932, test_loss.data.item()=2.6971\n",
      "24 loss.data.item()=2.5157, val_loss.data.item()=2.6894, test_loss.data.item()=2.6933\n",
      "25 loss.data.item()=2.5123, val_loss.data.item()=2.6858, test_loss.data.item()=2.6897\n",
      "26 loss.data.item()=2.5091, val_loss.data.item()=2.6825, test_loss.data.item()=2.6864\n",
      "27 loss.data.item()=2.5062, val_loss.data.item()=2.6794, test_loss.data.item()=2.6833\n",
      "28 loss.data.item()=2.5035, val_loss.data.item()=2.6764, test_loss.data.item()=2.6804\n",
      "29 loss.data.item()=2.5009, val_loss.data.item()=2.6737, test_loss.data.item()=2.6777\n",
      "30 loss.data.item()=2.4985, val_loss.data.item()=2.6711, test_loss.data.item()=2.6751\n",
      "31 loss.data.item()=2.4962, val_loss.data.item()=2.6687, test_loss.data.item()=2.6727\n",
      "32 loss.data.item()=2.4941, val_loss.data.item()=2.6664, test_loss.data.item()=2.6705\n",
      "33 loss.data.item()=2.4921, val_loss.data.item()=2.6643, test_loss.data.item()=2.6683\n",
      "34 loss.data.item()=2.4902, val_loss.data.item()=2.6622, test_loss.data.item()=2.6663\n",
      "35 loss.data.item()=2.4883, val_loss.data.item()=2.6603, test_loss.data.item()=2.6643\n",
      "36 loss.data.item()=2.4866, val_loss.data.item()=2.6584, test_loss.data.item()=2.6625\n",
      "37 loss.data.item()=2.4850, val_loss.data.item()=2.6566, test_loss.data.item()=2.6607\n",
      "38 loss.data.item()=2.4834, val_loss.data.item()=2.6550, test_loss.data.item()=2.6591\n",
      "39 loss.data.item()=2.4819, val_loss.data.item()=2.6534, test_loss.data.item()=2.6575\n",
      "40 loss.data.item()=2.4805, val_loss.data.item()=2.6518, test_loss.data.item()=2.6559\n",
      "41 loss.data.item()=2.4792, val_loss.data.item()=2.6504, test_loss.data.item()=2.6545\n",
      "42 loss.data.item()=2.4779, val_loss.data.item()=2.6490, test_loss.data.item()=2.6531\n",
      "43 loss.data.item()=2.4766, val_loss.data.item()=2.6476, test_loss.data.item()=2.6518\n",
      "44 loss.data.item()=2.4754, val_loss.data.item()=2.6463, test_loss.data.item()=2.6505\n",
      "45 loss.data.item()=2.4743, val_loss.data.item()=2.6451, test_loss.data.item()=2.6492\n",
      "46 loss.data.item()=2.4732, val_loss.data.item()=2.6439, test_loss.data.item()=2.6481\n",
      "47 loss.data.item()=2.4721, val_loss.data.item()=2.6427, test_loss.data.item()=2.6469\n",
      "48 loss.data.item()=2.4711, val_loss.data.item()=2.6416, test_loss.data.item()=2.6458\n",
      "49 loss.data.item()=2.4701, val_loss.data.item()=2.6406, test_loss.data.item()=2.6448\n"
     ]
    }
   ],
   "source": [
    "# init and training for bigram model\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "for i in range(50):\n",
    "    # forward\n",
    "    inputs = torch.nn.functional.one_hot(train_xs_bigram).float()\n",
    "    logits = inputs @ W \n",
    "    probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(train_ys_bigram.shape[0]), train_ys_bigram].log().mean()\n",
    "    \n",
    "    val_inputs = torch.nn.functional.one_hot(val_xs_bigram).float()\n",
    "    logits = val_inputs @ W\n",
    "    probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "    val_loss = -probs[torch.arange(val_ys_bigram.shape[0]), val_ys_bigram].log().mean()\n",
    "    \n",
    "    \n",
    "    print(i, f\"{loss.data.item()=:.4f}, {val_loss.data.item()=:.4f}, {test_loss.data.item()=:.4f}\")\n",
    "    \n",
    "    # backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # updates\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs_trigram, val_xs_trigram, test_xs_trigram = split_data(xs_trigram)\n",
    "train_ys_trigram, val_ys_trigram, test_ys_trigram = split_data(ys_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model):\n",
    "    trigram_input = \"..\"\n",
    "    output = \"\"\n",
    "    while True:\n",
    "        model_input = torch.tensor([[ltoi[x] for x in trigram_input]])\n",
    "        probs, _ = model(model_input)\n",
    "        next_letter = itol[torch.multinomial(probs, 1).item()]\n",
    "        if next_letter == \".\":\n",
    "            break\n",
    "        output += next_letter\n",
    "        trigram_input = output[-2:]\n",
    "        \n",
    "        if len(trigram_input) < 2:\n",
    "            trigram_input = \".\" + trigram_input\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, reg_weight=0):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn((27, 27)))\n",
    "        self.w2 = nn.Parameter(torch.randn((27, 27)))\n",
    "        self.reg_weight = reg_weight\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        inputs = torch.nn.functional.one_hot(x, num_classes=27).float()\n",
    "        logits = inputs[:, 0, :] @ self.w1 + inputs[:, 1, :] @ self.w2\n",
    "        probs = torch.exp(logits) / torch.exp(logits).sum(1, keepdims=True)\n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = -probs[torch.arange(targets.shape[0]), targets].log().mean()\n",
    "            \n",
    "            if self.reg_weight:\n",
    "                loss += self.reg_weight * torch.cat((self.w1**2, self.w2**2)).sum()\n",
    "\n",
    "        return probs, loss\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.w1.data += -lr * self.w1.grad\n",
    "        self.w2.data += -lr * self.w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5851, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probs, loss = m(train_xs_trigram, targets=train_ys_trigram)\n",
    "m.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.update(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5196, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, loss = m(train_xs_trigram, targets=train_ys_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7461421489715576"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg=0.001 loss.data.item()=2.6747 val_loss.data.item()=2.8234 test_loss.data.item()=2.8281\n"
     ]
    }
   ],
   "source": [
    "for reg in [1e-3]: #[0, 1e-10, 1e-5, 1e-3]:\n",
    "    m = Model(reg_weight=reg)\n",
    "    for i in range(50):\n",
    "        probs, loss = m(train_xs_trigram, targets=train_ys_trigram)\n",
    "        _, val_loss = m(val_xs_trigram, targets=val_ys_trigram)\n",
    "        _, test_loss = m(test_xs_trigram, targets=test_ys_trigram)\n",
    "        \n",
    "        # print(i, f\"{loss.data.item()=:.4f}, {val_loss.data.item()=:.4f}\")\n",
    "        \n",
    "        m.zero_grad()\n",
    "        loss.backward()\n",
    "        m.update(20)\n",
    "\n",
    "    print(f\"{reg=}\", f\"{loss.data.item()=:.4f}\", f\"{val_loss.data.item()=:.4f}\", f\"{test_loss.data.item()=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daca'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "\n",
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n",
    "\n",
    "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
